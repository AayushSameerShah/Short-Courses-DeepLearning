# ðŸˆ Different Paradigm

> Instead of having a **hold out set beforehand** as we usually have in the standard ML, here we **gradually <u>build</u> the test set** ðŸ˜Š

**Some starter questions:**

- After you've built the system, how would you know how is it working after you've deployed

- How would you keep track of its performance once the users start using it?

- How will we continue to improve?

# ðŸ‘‰ðŸ» We have started building the model with "zero" training examples

LLMs save a lot of time for building a model targeted for a certain usecase, but **the main pain** is to gather enough data to *test* the model's performance.

## ðŸ©º The LLM Diagnostics

1. Tune prompts **on handful of examples**.

2. **Add** additional "tricky" examples ***"opportunistically"***.

3. **Develop metrics** to measure performance on examples.

4. **Collect randomly sampled set** of examples to tune to *(development set/hold-out cross validation set).*

5. Collect and **use a hold-out** test set *(that you haven't looked your self!)*

> The main **advantage** here is, on any moment that you think that your system is performing well, **then you stop on any bullet point** and continue to the deployment and so on.
> 
> â€” Andrew

### ðŸ—º So, the process might look like...

- Example-1 âœ”

- Example-2 âœ”

- Example-3 âŒ *(add in the test set)*

- Example-4 âœ” 

- ...

After this, tune the prompt and **test it on the test set**. 

Works good on test set?

Re-run all examples on this new prompt a.k.a. **regression testing** to make sure the old examples aren't messed up.

# ðŸ¤– Make a function to do testing

Things will get "repetative" â€” make a function that can run all samples through the model and get the result **and also score the result**; since we are not going to eyeball those samples!

> ðŸ‘‰ðŸ» Now this is important, **scoring the output** enables us to compare different prompts and models across wide range of tasks!

# ðŸ›  An Epic Tool: Rubric

When we **don't have a single "right" answer** we need some sophesticated metric. Usually in the LLMs, we use ***BLEU*** and ***Rouge*** scores â€” also the ***Ruberic*** that we will be using here; which simply is asking the LLM to *step-by-step* reason through its output.

> **Rubric**: A set of guidelines to test the response from multiple dimensions.

## Example-1:

```markdown
[SYS]
You are an assistant that evaluates how well the customer service 
agent answers a user question by looking at the context that the
customer service agent is using to generate its response. 

[USR]
You are evaluating a submitted answer to a question based on the 
context that the agent uses to answer the question.
Here is the data:

<<BEGIN DATA>>
************
<<Question>>: 
tell me about the smartx pro phone and the fotosnap camera, 
the dslr one. Also, what TVs or TV related products do you have?
************
<<Context>>: 
[{
'name': 'SmartX ProPhone', 
'category': 'Smartphones and Accessories', 
... }]
************
<<Submission>>: 
Sure! Let me provide you with some information about the SmartX ProPhone 
and the FotoSnap DSLR Camera. The SmartX ProPhone is a powerful smartphone 
with advanced camera features. It has a 6.1-inch display, 128GB storage, 
a 12MP dual camera, and supports 5G connectivity. The SmartX ProPhone is 
priced at $899.99 and comes with a 1-year warranty.

The FotoSnap DSLR Camera is a versatile camera that allows you to capture 
stunning photos and videos. It features a 24.2MP sensor, 1080p video 
recording, a 3-inch LCD screen, and supports interchangeable lenses. 
The FotoSnap DSLR Camera is priced at $599.99 and also comes with a 
1-year warranty.
************
<<END DATA>>

**Compare the factual content of the submitted answer with the context. 
Ignore any differences in style, grammar, or punctuation.**
Answer the following questions:
- Is the Assistant response based only on the context provided? (Y or N)
- Does the answer include information that is not provided in the context? (Y or N)
- Is there any disagreement between the response and the context? (Y or N)
- Count how many questions the user asked. (output a number)
- For each question that the user asked, is there a corresponding answer to it?
      Question 1: (Y or N)
      Question 2: (Y or N)
      ...
      Question N: (Y or N)
- Of the number of questions asked, how many of these questions were addressed by the answer? (output a number)
```

ðŸ¤– The typical response would be:

```
- Is the Assistant response based only on the context provided? (Y or N)
Y

- Does the answer include information that is not provided in the context? (Y or N)
N

- Is there any disagreement between the response and the context? (Y or N)
N

- Count how many questions the user asked. (output a number)
2

- For each question that the user asked, is there a corresponding answer to it?
Question 1: Y
Question 2: Y

- Of the number of questions asked, how many of these questions were addressed by the answer? (output a number)
2
```

ðŸ“¦ **So to summarize as a TL;DR the given rubric above does the following:**

- Checks if the response of the assistance is **based on the context provided only**?

- Any information included in the response **which is out of context**?

- Any **disagreement** between the response and the context?

- Counting the number of questions asked by the user (in number).

- For each question the user has asked, is there answer to each question in the response?

## Example - 2

```markdown
The submitted answer may either be a subset or superset of the expert answer, or it may conflict with it. Determine which case applies. Answer the question by selecting one of the following options:
(A) The submitted answer is a subset of the expert answer and is fully consistent with it.
(B) The submitted answer is a superset of the expert answer and is fully consistent with it.
(C) The submitted answer contains all the same details as the expert answer.
(D) There is a disagreement between the submitted answer and the expert answer.
(E) The answers differ, but these differences don't matter from the perspective of factuality.
choice_strings: ABCDE
```

This one is used to **compare the ideal answer with the LLM response.**

>  *Please also checkout the [Evals Framework]([openai/evals: Evals is a framework for evaluating LLMs and LLM systems, and an open-source registry of benchmarks. (github.com)](https://github.com/openai/evals)) by OpenAI.*
