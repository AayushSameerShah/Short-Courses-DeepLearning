# ğŸŒµ Feature Selection Techniques

There are certain techniques to go over:

1. All features in

2. Backward elimination

3. Forward selection

4. Bidirectional elimination

5. All combinations

### `1.` All features in

Just the dumb way, put all in.

### `2.` Backward elimination

1. Put all features in first.

2. Check the `p` value.

3. If it is **higher** than the set threshold (more then 0.05 - which is not significant) then remove that feature

4. In the step 3 we will sort the features descending order, and remove the first only. Not multiple.

5. Build the model and repeat the 3,4,5 steps until all features have significance less than 0.05 ğŸ¤—

### `3.` Forward Selection

1. Start with all features in.

2. **Sort** the features from lowest, if the lowest feature has less than 0.05 significance, then select that feature.

3. Build the model taking other feature with this selected feature.
   
   > Example: Suppose there are total `10` features in the dataset, you will make a model with all 10 features. From that, feature `A` has lowest p value, you select that. 
   > 
   > - Then with this `A` selected you take other `9` features one by one say... `AB`, `AC`, `AD` etc. 
   > 
   > - Then you will have other 9 models ready? Good.
   > 
   > - Now, from these 9 models pick the model which has lowest p value of the second feature - say `AD` came out to be that one.
   > 
   > - Now repeat the process, make `8` models with `AD` already selected. So, you will keep making `ADB`, `ADC` ... you know?
   > 
   > - Keep this process running until either you have tried all features or any additional feature doesn't help.
   
   4. I think the example solves all, no need 4th step now ğŸ¤—
   
   ### `4.` Bidirectional Elimination
   
   1. Select `2` **(two)** significance levels. One for adding a variable and other for removing a variable. (Say we make 2 levels `LEVEL ENTER` and `LEVEL EXIT`)
   
   2. Add all features and make the model.
   
   3. Select the feature with lowest `p` value. (It should be lower than `LEVEL ENTER`)
   
   4. Keep that one and make another model just like forward selection `AB`, `AC` etc.
   
   5. Now, check the `p` value of all feature **and remove the single feature which has `p` value higher than `LEVEL EXIT`**.
   
   6. Now, continue the 3, 4, 5 steps ğŸ¤—

### `5.` All possible models

Here all possible combinations of features $2^n - 1$.

# 0ï¸âƒ£1ï¸âƒ£ Dummy variables are switches

When we add the dummy or one-hot encoded variables, they act like switches (of course we make $N-1$ variables to avoid multicollinearity)

But when there is `1`, it simply indicates the presence of the certain categorical value, **so the coefficient will only work when the value is `1` and when that is `0` the coef will simply be zero!**. 

> ## ğŸ€
> 
> Here is a good part, the other variable's coef **isn't lost** it is included in the ***intercept***! So keep that in mind.

# â¤´ Polynomial regression

It follows just the same formula as multiple reg, but it has **different forms** of the same variable in the different powers ğŸ’ªğŸ».

> ğŸ’¡ **Why is it still called "Linear"?**
> 
> See, when we talk about the term linear, we don't talk about the variable being powered, or multiplied etc, ***but how the coefficients are internally connected***. Because the "knowledge" is the coefficients and they are linearly or additively connected to get the `y`.

**Snippet:**

```python
from sklearn.preprocessing import PolynomialFeatures


trans = PolynomialFeatures(degree=2)
trans.fit_transform(X["the_feat"])
```

# â— Also scale `y`

When we are dealing with the **equation based** model, applying scaling to the features is not enough; we also need to apply the scaling to the target. There ***two scalers will be used*** ğŸ¤—

And then we are done with the predictions, we will **need to redo the transformation to bring the `y` back to the original scale!**.

# ğŸ’³ The available SVM kernels

1. Polynomial kernel

2. Gaussian kernel

3. Gaussian Radial Basis kernel

4. Laplace RBS kernel

5. Hyperbolic Tangent kernel

6. Sigmoid kernel

7. Bessel Function of First Kind kernel (WTF!?)

8. Anova Radial Basis kernel

9. Linear spline kernel in 1d

***Huff!!***
