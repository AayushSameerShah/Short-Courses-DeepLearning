# 💰 Multi-Armed Bandit

![](E:\GitHub\Short-Courses-DeepLearning\Machine-Learning-A-Z\images\multiarmed-bandit.png)

Here the task is **to find the most optimal machine or distribution which maximizes the result**.

- So, suppose there are `5` machines, that exist. 

- The ***irony*** is that we don't know their distribution because, you know otherwise we wouldn't even consider doing this experiment.

- So, we need toe **come up with a plan** which allows us to find the most optimal machine with the least "regret" *(it is a term!)*.

- Where we spend least amount of time and money and also don't end up at suboptimal machine (a machine which *looks like* optimal but isn't originally optimal).

## 🍾 The Coca-Cola ad

![](E:\GitHub\Short-Courses-DeepLearning\Machine-Learning-A-Z\images\coke-ad.png)

- This is coolest example.

- Coke wants to know which ad will gain the highest revenue, **and the irony** is they don't know (the hidden distribution).

- So the need to run a strategy.

## 🆎 The A/B doesn't work...

Because, yes here we tests all options, but **the exploration and exploitation** gets the same efforts! *(Exploration is finding and exploitation is reaching to the optimal one)*.

> ## 💡
> 
> Here, in A/B test if we do the 1000 trials in total for 2 strategies, say then both will get 500, 500 chances. So, the test distribution is uniform and we will be ***wasting*** the time and money on strategy A which is bad!

#### 👉🏻 We need a way to <u>Exploit</u>, while <u>Exploring </u>👈🏻

# Different Methods for Exploration and Exploitation

- 1️⃣ Epsilon-greedy, 

- 2️⃣ Softmax

- 3️⃣ Upper confidence bound (UCB)

- 4️⃣ Thompson sampling.

There can be more but I have listed these based on the [article](https://www.linkedin.com/advice/1/how-do-you-optimize-your-strategy-exploration#how-do-you-implement-it?).

## 🎛 Upper Confidence Bound (UCB)

This is a cool algorithm, really. So, let's just see the intuition.

1. Say there are 5 ad strategies

2. Initially we will start with some "average" return and each ad will get the **same return**.

3. Each return will have the "confidence bound".

4. Then we take the random ad and apply it, if it gives positive outcome, we shift the average and if negative we shift the average down.

5. On **each iteration** the confidence bound becomes smaller because now we have more observation to prove the point.

6. The next machine is chosen based **on some strategy**. That can be decided *(I don't have clarity, but the algorithm choses the machine which **has highest return till now** and has more chances to getting selected in the coming tests than other machines — <u>thus exploration with exploitation</u>).*

> ## 🔥
> 
> **The UCB Mechanism of Sampling Next AD**:
> 
> Yes, I told that "I don't know" *just above*, but now I have a fair idea on this as how this works. This [Best Multi-Armed Bandit Strategy? (feat: UCB Method)](https://www.youtube.com/watch?v=FgmMK6RPU1c) is at rescue. 
> 
> - After the initial average, we have started exploring the ads, by random, okay?
> 
> - Now, we will **keep track of the following term:**
>   
>   - $[\mu_\text{ad\_i} + \sqrt{\frac{2 ln(t)}{N_t(\text{ad\_i})}}]$
> 
> - The above formula is run for every ad on the time `t`. (So basically we will be testing this for months, right? So `t` is an individual day where only one test is carried out.)
> 
> - The first half keeps track of **the found mean** and then adds the balancing term - the second half.
> 
> - That simply divides ***that ad's total visit till now*** so that the other unvisited ads also get chance to be chosen.
> 
> - And finally by ***weighted random*** we choose the ad with highest value.

### 💥 While choosing a right method...

... we also need to keep in mind the `time - t` available. Because UCB method needs some more time to be used.

# 🤚🏻 ASSUMPTION

These all techniques assume that "the conversion rate is fixed". **Means, their distributions don't change over time.**

> This is the type of online learning 😉 

# 👡 Thompson Sampling

- Here, we will follow the similar steps, but the algorithm formulates the **distribution for each ad/machines** and works based on that.

- In contrast to the UCB, we calculated the rewards and mean and all  to select the next machine etc. Here, we will sample from the distribution to select the next machine. 

The hit line:

> 👉🏻 ***With thompson sampling, we are not trying to guess the underlying distributions for each ads/machines.***
> 
> 👉🏻 **Thompson sampling** is the *probabilistic* algorithm when the **UCB** is the *deterministic* algorithm.
