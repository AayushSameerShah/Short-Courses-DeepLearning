# Gradient Descent

The simple way to get to the optimal weights by constantly looking at the loss. **But this way** you may reach to the local minima, and not the global minima ***if the cost function is not convex.*** 

> *Which means, the GD algorithm requires the function to be convex, meaning it should have just the `U` shape. If the function is not the convex, meaning some other shape like `W` then Gradient Descent will reach to the wrong set of weights.*

## So, the Stochastic Gradient Descent

> This doesn't require the cost function to be convex.

So, it feeds and trains the NN by choosing sub-set of data, so that it is less prone to reach at the local minima. 

- If you start with the model and train with the stochastic, then the end weights may not be the same because it trains based on the subset of data.

- Batch-gradient descent or the simple gradient descent (first one) will **always** result in the same NN weights.

> *Skipping a lot of basic part that this course doesn't go over in proper depth, and will discuss that later when we will take some serious course.*

# ðŸ•¸ Dimensionality Reduction

In this part we will cover the following ***Feature Extraction*** techniques:

1. Principal Component Analysis (PCA)
2. Linear Discriminant Analysis (LDA)
3. Kernel PCA
4. Quadratic Discriminant Analysis (QDA)

> ***It is,*** you find the maximum directions of variance in the high dimensional data, and then project in the smaller dimension subspace â€” while retaining most of the information.

## Linear Discriminant Analysis (LDA)

> - LDA differs because in addition to finding the component axes with LDA we are interested in the **axes that maximize the separation between multiple classes**.
>   
> 
> - Both PCA and LDA are linear transformation techniques used for dimensional reduction. PCA is described as unsupervised but **LDA is
>   supervised** because of the relation to the dependent variable.

## Kernel PCA!

This allows **to select the kernel** like `rbf` and other... just nothing. Skip this.



# Logistic Regression!

Linear Regression:
$y = b_0 + b_1 \cdot x$



Softmax Function:
$p = \frac{1}{1 + e^{-y}}$



Logistic Regression (Log-Odds Form):
$\ln\left(\frac{p}{1 - p}\right) = b_0 + b_1 \cdot x $
